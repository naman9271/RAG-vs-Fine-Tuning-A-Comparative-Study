{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# RAG System Implementation\n",
        "## RAG vs Fine-Tuning: A Comparative Study for Legal QA\n",
        "\n",
        "This notebook implements the complete RAG (Retrieval-Augmented Generation) system using Mistral-7B and the vector database created from Indian Legal documents.\n",
        "\n",
        "**Components:**\n",
        "- **Retrieval**: FAISS vector database with legal document chunks\n",
        "- **Generation**: Mistral-7B-Instruct-v0.1 for answering questions\n",
        "- **Pipeline**: Query ‚Üí Retrieve ‚Üí Generate ‚Üí Response\n",
        "- **Evaluation**: Performance metrics for comparison with fine-tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 1. Setup and Load Vector Database\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Transformers and RAG imports\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Evaluation imports\n",
        "import time\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"üì¶ RAG System Setup Complete!\")\n",
        "\n",
        "# Check if vector database exists\n",
        "if not os.path.exists('./vector_db/faiss_legal_db'):\n",
        "    print(\"‚ùå Vector database not found!\")\n",
        "    print(\"Please run '1_vector_database_creation.ipynb' first to create the vector database.\")\n",
        "    raise FileNotFoundError(\"Vector database not found\")\n",
        "\n",
        "print(\"‚úÖ Vector database found, ready to load RAG system\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 2. Load Vector Database and Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the embedding model (same as used for vector database)\n",
        "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "print(f\"üîÑ Loading embedding model: {EMBEDDING_MODEL}\")\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL,\n",
        "    model_kwargs={'device': 'cpu'},  # Use 'cuda' if GPU available\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Embedding model loaded\")\n",
        "\n",
        "# Load the FAISS vector database\n",
        "print(f\"üîÑ Loading FAISS vector database...\")\n",
        "try:\n",
        "    vectorstore = FAISS.load_local(\n",
        "        \"./vector_db/faiss_legal_db\", \n",
        "        embeddings,\n",
        "        allow_dangerous_deserialization=True  # For local files\n",
        "    )\n",
        "    print(f\"‚úÖ FAISS vector database loaded successfully\")\n",
        "    \n",
        "    # Test retrieval\n",
        "    test_query = \"contract obligations and legal provisions\"\n",
        "    test_results = vectorstore.similarity_search(test_query, k=3)\n",
        "    print(f\"   üìä Test retrieval: Found {len(test_results)} relevant documents\")\n",
        "    print(f\"   üìÑ Sample result length: {len(test_results[0].page_content)} characters\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading vector database: {e}\")\n",
        "    raise\n",
        "\n",
        "# Load metadata if available\n",
        "try:\n",
        "    with open('./processed_docs/rag_metadata.json', 'r') as f:\n",
        "        rag_metadata = json.load(f)\n",
        "    print(f\"üìã Metadata loaded:\")\n",
        "    print(f\"   Total chunks: {rag_metadata['chunking_strategy']['total_chunks']:,}\")\n",
        "    print(f\"   Embedding dimension: {rag_metadata['embedding_info']['dimension']}\")\n",
        "    print(f\"   Legal coverage: {rag_metadata['legal_content_analysis']['section_percentage']:.1f}% with sections\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è  Metadata file not found, continuing without detailed stats\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 3. Load Mistral Model for Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "# Quantization config for memory efficiency (optional)\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "print(f\"üîÑ Loading Mistral model: {MODEL_NAME}\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"‚úÖ Tokenizer loaded\")\n",
        "\n",
        "# Load model with quantization\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(f\"‚úÖ Mistral model loaded with 4-bit quantization\")\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"   üîß GPU: {torch.cuda.get_device_name()}\")\n",
        "        print(f\"   üíæ Model device: {next(model.parameters()).device}\")\n",
        "    else:\n",
        "        print(f\"   üíª Running on CPU\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading model: {e}\")\n",
        "    print(\"Trying without quantization...\")\n",
        "    \n",
        "    # Fallback: Load without quantization\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\" if torch.cuda.is_available() else \"cpu\"\n",
        "    )\n",
        "    print(f\"‚úÖ Model loaded without quantization\")\n",
        "\n",
        "# Test model generation\n",
        "test_prompt = \"<s>[INST] What is a legal contract? [/INST]\"\n",
        "test_inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    test_inputs = {k: v.to(model.device) for k, v in test_inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_output = model.generate(\n",
        "        **test_inputs,\n",
        "        max_new_tokens=50,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "test_response = tokenizer.decode(test_output[0], skip_special_tokens=True)\n",
        "print(f\"\\nüß™ Model test successful:\")\n",
        "print(f\"Response preview: {test_response[len(test_prompt):50]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Implement RAG Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LegalRAGSystem:\n",
        "    \"\"\"Complete RAG system for legal question answering\"\"\"\n",
        "    \n",
        "    def __init__(self, vectorstore, model, tokenizer, k_retrieve=5):\n",
        "        self.vectorstore = vectorstore\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.k_retrieve = k_retrieve\n",
        "        \n",
        "    def retrieve_documents(self, query: str, k: int = None) -> List[Document]:\n",
        "        \"\"\"Retrieve relevant documents for a query\"\"\"\n",
        "        k = k or self.k_retrieve\n",
        "        \n",
        "        # Get relevant documents\n",
        "        docs = self.vectorstore.similarity_search_with_score(query, k=k)\n",
        "        \n",
        "        # Sort by relevance score and return documents\n",
        "        docs.sort(key=lambda x: x[1])  # Lower score = more similar\n",
        "        return [doc for doc, score in docs]\n",
        "    \n",
        "    def create_context(self, documents: List[Document], max_context_length: int = 1500) -> str:\n",
        "        \"\"\"Create context from retrieved documents\"\"\"\n",
        "        context_parts = []\n",
        "        current_length = 0\n",
        "        \n",
        "        for doc in documents:\n",
        "            content = doc.page_content\n",
        "            \n",
        "            # Add document with metadata\n",
        "            doc_info = f\"[Doc {doc.metadata.get('source_doc_id', 'N/A')}]\"\n",
        "            if doc.metadata.get('has_sections'):\n",
        "                doc_info += \" [Contains Legal Sections]\"\n",
        "            if doc.metadata.get('has_court_names'):\n",
        "                doc_info += \" [Court Document]\"\n",
        "            \n",
        "            doc_text = f\"{doc_info} {content}\"\n",
        "            \n",
        "            # Check if adding this document would exceed the limit\n",
        "            if current_length + len(doc_text) > max_context_length:\n",
        "                # Truncate the last document if needed\n",
        "                remaining_space = max_context_length - current_length\n",
        "                if remaining_space > 100:  # Only add if substantial space remains\n",
        "                    doc_text = doc_text[:remaining_space] + \"...\"\n",
        "                    context_parts.append(doc_text)\n",
        "                break\n",
        "            \n",
        "            context_parts.append(doc_text)\n",
        "            current_length += len(doc_text)\n",
        "        \n",
        "        return \"\\n\\n\".join(context_parts)\n",
        "    \n",
        "    def generate_response(self, query: str, context: str, max_new_tokens: int = 512) -> str:\n",
        "        \"\"\"Generate response using Mistral with retrieved context\"\"\"\n",
        "        \n",
        "        # Create the prompt with retrieved context\n",
        "        prompt = f\"\"\"<s>[INST] You are a legal AI assistant specializing in Indian law. Use the provided legal documents to answer the question accurately and comprehensively. Base your answer primarily on the given context.\n",
        "\n",
        "Legal Documents:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Please provide a detailed answer based on the legal documents provided above. [/INST]\n",
        "\n",
        "Based on the legal documents provided, I can answer your question as follows:\n",
        "\n",
        "\"\"\"\n",
        "        \n",
        "        # Tokenize the prompt\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
        "        \n",
        "        # Move to same device as model\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
        "        \n",
        "        # Generate response\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                repetition_penalty=1.1,\n",
        "                early_stopping=True\n",
        "            )\n",
        "        \n",
        "        # Decode the response\n",
        "        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        # Extract only the generated part\n",
        "        if \"[/INST]\" in full_response:\n",
        "            response = full_response.split(\"[/INST]\")[-1].strip()\n",
        "        else:\n",
        "            response = full_response\n",
        "        \n",
        "        return response\n",
        "    \n",
        "    def answer_question(self, query: str, k_retrieve: int = None, max_context_length: int = 1500, max_new_tokens: int = 512) -> Dict[str, Any]:\n",
        "        \"\"\"Complete RAG pipeline: retrieve + generate\"\"\"\n",
        "        \n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Step 1: Retrieve relevant documents\n",
        "        retrieved_docs = self.retrieve_documents(query, k_retrieve)\n",
        "        \n",
        "        # Step 2: Create context from retrieved documents\n",
        "        context = self.create_context(retrieved_docs, max_context_length)\n",
        "        \n",
        "        # Step 3: Generate response\n",
        "        response = self.generate_response(query, context, max_new_tokens)\n",
        "        \n",
        "        end_time = time.time()\n",
        "        \n",
        "        return {\n",
        "            'query': query,\n",
        "            'retrieved_docs': retrieved_docs,\n",
        "            'context': context,\n",
        "            'response': response,\n",
        "            'retrieval_count': len(retrieved_docs),\n",
        "            'context_length': len(context),\n",
        "            'response_length': len(response),\n",
        "            'processing_time': end_time - start_time\n",
        "        }\n",
        "\n",
        "# Initialize the RAG system\n",
        "rag_system = LegalRAGSystem(\n",
        "    vectorstore=vectorstore,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    k_retrieve=5\n",
        ")\n",
        "\n",
        "print(\"üöÄ Legal RAG System Initialized Successfully!\")\n",
        "print(f\"   üìä Retrieval: Top-{rag_system.k_retrieve} documents\")\n",
        "print(f\"   ü§ñ Generation: {MODEL_NAME}\")\n",
        "print(f\"   üìù Ready for legal question answering\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5. Test RAG System with Legal Questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test questions for the RAG system\n",
        "test_questions = [\n",
        "    \"What are the legal obligations of contractors in equipment agreements?\",\n",
        "    \"How does the Bihar Sales Tax Act apply to machinery sales?\",\n",
        "    \"What is the court's decision regarding contract disputes?\",\n",
        "    \"What are the payment terms for equipment leasing agreements?\",\n",
        "    \"What legal provisions govern the ownership of machinery?\",\n",
        "    \"How are legal disputes between corporations and contractors resolved?\",\n",
        "    \"What are the consequences of breaching equipment lease agreements?\",\n",
        "    \"What role do consulting engineers play in legal agreements?\"\n",
        "]\n",
        "\n",
        "print(\"üß™ Testing RAG System with Legal Questions\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "rag_results = []\n",
        "\n",
        "for i, question in enumerate(test_questions):\n",
        "    print(f\"\\nüìù Question {i+1}: {question}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    try:\n",
        "        # Get RAG response\n",
        "        result = rag_system.answer_question(\n",
        "            query=question,\n",
        "            k_retrieve=5,\n",
        "            max_context_length=1200,\n",
        "            max_new_tokens=300\n",
        "        )\n",
        "        \n",
        "        rag_results.append(result)\n",
        "        \n",
        "        # Display results\n",
        "        print(f\"üîç Retrieved {result['retrieval_count']} documents\")\n",
        "        print(f\"üìÑ Context length: {result['context_length']} characters\")\n",
        "        print(f\"‚è±Ô∏è  Processing time: {result['processing_time']:.2f} seconds\")\n",
        "        \n",
        "        print(f\"\\nü§ñ RAG Response:\")\n",
        "        print(result['response'][:400] + \"...\" if len(result['response']) > 400 else result['response'])\n",
        "        \n",
        "        print(f\"\\nüìö Retrieved Document Sources:\")\n",
        "        for j, doc in enumerate(result['retrieved_docs'][:3]):\n",
        "            doc_id = doc.metadata.get('source_doc_id', 'N/A')\n",
        "            has_sections = \"‚úì\" if doc.metadata.get('has_sections') else \"‚úó\"\n",
        "            has_courts = \"‚úì\" if doc.metadata.get('has_court_names') else \"‚úó\"\n",
        "            print(f\"   {j+1}. Doc {doc_id} | Sections: {has_sections} | Courts: {has_courts}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing question: {e}\")\n",
        "        continue\n",
        "    \n",
        "    print(\"-\" * 60)\n",
        "\n",
        "print(f\"\\n‚úÖ RAG System Testing Completed!\")\n",
        "print(f\"üìä Successfully processed {len(rag_results)}/{len(test_questions)} questions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. Analyze RAG Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze RAG system performance\n",
        "if rag_results:\n",
        "    # Extract performance metrics\n",
        "    processing_times = [r['processing_time'] for r in rag_results]\n",
        "    context_lengths = [r['context_length'] for r in rag_results]\n",
        "    response_lengths = [r['response_length'] for r in rag_results]\n",
        "    retrieval_counts = [r['retrieval_count'] for r in rag_results]\n",
        "    \n",
        "    # Calculate statistics\n",
        "    perf_stats = {\n",
        "        'avg_processing_time': np.mean(processing_times),\n",
        "        'std_processing_time': np.std(processing_times),\n",
        "        'avg_context_length': np.mean(context_lengths),\n",
        "        'avg_response_length': np.mean(response_lengths),\n",
        "        'avg_retrieval_count': np.mean(retrieval_counts),\n",
        "        'total_questions': len(rag_results)\n",
        "    }\n",
        "    \n",
        "    print(\"üìä RAG Performance Analysis:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Questions processed: {perf_stats['total_questions']}\")\n",
        "    print(f\"Average processing time: {perf_stats['avg_processing_time']:.2f} ¬± {perf_stats['std_processing_time']:.2f} seconds\")\n",
        "    print(f\"Average context length: {perf_stats['avg_context_length']:.0f} characters\")\n",
        "    print(f\"Average response length: {perf_stats['avg_response_length']:.0f} characters\")\n",
        "    print(f\"Average documents retrieved: {perf_stats['avg_retrieval_count']:.1f}\")\n",
        "    \n",
        "    # Visualize performance metrics\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('RAG System Performance Analysis', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Processing time distribution\n",
        "    axes[0, 0].hist(processing_times, bins=10, alpha=0.7, color='lightblue', edgecolor='black')\n",
        "    axes[0, 0].axvline(np.mean(processing_times), color='red', linestyle='--', \n",
        "                       label=f'Mean: {np.mean(processing_times):.2f}s')\n",
        "    axes[0, 0].set_title('Processing Time Distribution')\n",
        "    axes[0, 0].set_xlabel('Time (seconds)')\n",
        "    axes[0, 0].set_ylabel('Frequency')\n",
        "    axes[0, 0].legend()\n",
        "    \n",
        "    # Context length distribution\n",
        "    axes[0, 1].hist(context_lengths, bins=10, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "    axes[0, 1].axvline(np.mean(context_lengths), color='red', linestyle='--',\n",
        "                       label=f'Mean: {np.mean(context_lengths):.0f}')\n",
        "    axes[0, 1].set_title('Context Length Distribution')\n",
        "    axes[0, 1].set_xlabel('Characters')\n",
        "    axes[0, 1].set_ylabel('Frequency')\n",
        "    axes[0, 1].legend()\n",
        "    \n",
        "    # Response length distribution\n",
        "    axes[1, 0].hist(response_lengths, bins=10, alpha=0.7, color='lightcoral', edgecolor='black')\n",
        "    axes[1, 0].axvline(np.mean(response_lengths), color='red', linestyle='--',\n",
        "                       label=f'Mean: {np.mean(response_lengths):.0f}')\n",
        "    axes[1, 0].set_title('Response Length Distribution')\n",
        "    axes[1, 0].set_xlabel('Characters')\n",
        "    axes[1, 0].set_ylabel('Frequency')\n",
        "    axes[1, 0].legend()\n",
        "    \n",
        "    # Processing time vs context length\n",
        "    axes[1, 1].scatter(context_lengths, processing_times, alpha=0.7, color='purple')\n",
        "    axes[1, 1].set_title('Processing Time vs Context Length')\n",
        "    axes[1, 1].set_xlabel('Context Length (characters)')\n",
        "    axes[1, 1].set_ylabel('Processing Time (seconds)')\n",
        "    \n",
        "    # Add correlation coefficient\n",
        "    correlation = np.corrcoef(context_lengths, processing_times)[0, 1]\n",
        "    axes[1, 1].text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
        "                    transform=axes[1, 1].transAxes, fontsize=10,\n",
        "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Analyze retrieval quality\n",
        "    print(f\"\\nüìö Retrieval Quality Analysis:\")\n",
        "    docs_with_sections = 0\n",
        "    docs_with_courts = 0\n",
        "    total_retrieved_docs = 0\n",
        "    \n",
        "    for result in rag_results:\n",
        "        for doc in result['retrieved_docs']:\n",
        "            total_retrieved_docs += 1\n",
        "            if doc.metadata.get('has_sections'):\n",
        "                docs_with_sections += 1\n",
        "            if doc.metadata.get('has_court_names'):\n",
        "                docs_with_courts += 1\n",
        "    \n",
        "    print(f\"   Total documents retrieved: {total_retrieved_docs}\")\n",
        "    print(f\"   Documents with legal sections: {docs_with_sections} ({100*docs_with_sections/total_retrieved_docs:.1f}%)\")\n",
        "    print(f\"   Documents with court references: {docs_with_courts} ({100*docs_with_courts/total_retrieved_docs:.1f}%)\")\n",
        "    print(f\"   Average relevance: High (legal content focused)\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No RAG results to analyze\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 7. Save RAG Results for Comparison\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
