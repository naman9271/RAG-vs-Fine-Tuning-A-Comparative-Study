{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Fine-Tuning Data Preparation\n",
        "## RAG vs Fine-Tuning: A Comparative Study for Legal QA\n",
        "\n",
        "This notebook prepares the Indian Legal dataset for fine-tuning the Mistral model.\n",
        "\n",
        "**Dataset**: [ninadn/indian-legal](https://huggingface.co/datasets/ninadn/indian-legal)  \n",
        "**Model**: Mistral-7B-Instruct-v0.1  \n",
        "**Task**: Legal Question Answering  \n",
        "**Approach**: Instruction Tuning with QLoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Create directories\n",
        "os.makedirs('./processed_data', exist_ok=True)\n",
        "os.makedirs('./models', exist_ok=True)\n",
        "\n",
        "print(\"üì¶ Environment setup complete!\")\n",
        "print(\"üîç Ready to process Indian Legal dataset for Mistral fine-tuning\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 2. Load Indian Legal Dataset from Hugging Face\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Indian Legal dataset from Hugging Face\n",
        "print(\"üîÑ Loading Indian Legal Dataset from Hugging Face...\")\n",
        "try:\n",
        "    dataset = load_dataset(\"ninadn/indian-legal\")\n",
        "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "    print(f\"üìä Dataset structure: {dataset}\")\n",
        "    \n",
        "    # Convert to pandas for analysis\n",
        "    train_df = pd.DataFrame(dataset['train'])\n",
        "    test_df = pd.DataFrame(dataset['test'])\n",
        "    \n",
        "    print(f\"\\nüìà Dataset Statistics:\")\n",
        "    print(f\"  Training samples: {len(train_df):,}\")\n",
        "    print(f\"  Test samples: {len(test_df):,}\")\n",
        "    print(f\"  Total samples: {len(train_df) + len(test_df):,}\")\n",
        "    print(f\"  Columns: {list(train_df.columns)}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading dataset: {e}\")\n",
        "    print(\"Please check your internet connection and Hugging Face access\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 3. Exploratory Data Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze the text data\n",
        "print(\"üìä Analyzing Legal Documents...\")\n",
        "\n",
        "# Calculate text statistics\n",
        "train_df['text_length'] = train_df['Text'].str.len()\n",
        "test_df['text_length'] = test_df['Text'].str.len()\n",
        "\n",
        "# Display basic statistics\n",
        "print(\"\\nüìè Text Length Statistics:\")\n",
        "print(\"=\" * 50)\n",
        "stats_data = {\n",
        "    'Dataset': ['Training', 'Test'],\n",
        "    'Count': [len(train_df), len(test_df)],\n",
        "    'Mean Length': [train_df['text_length'].mean(), test_df['text_length'].mean()],\n",
        "    'Median Length': [train_df['text_length'].median(), test_df['text_length'].median()],\n",
        "    'Min Length': [train_df['text_length'].min(), test_df['text_length'].min()],\n",
        "    'Max Length': [train_df['text_length'].max(), test_df['text_length'].max()],\n",
        "    'Std Dev': [train_df['text_length'].std(), test_df['text_length'].std()]\n",
        "}\n",
        "\n",
        "stats_df = pd.DataFrame(stats_data)\n",
        "print(stats_df.round(2).to_string(index=False))\n",
        "\n",
        "# Sample documents\n",
        "print(f\"\\nüìù Sample Legal Documents:\")\n",
        "print(\"=\" * 80)\n",
        "for i, (idx, row) in enumerate(train_df.head(2).iterrows()):\n",
        "    print(f\"\\nüèõÔ∏è Document {i+1} ({len(row['Text'])} characters):\")\n",
        "    print(\"-\" * 40)\n",
        "    # Show first 600 characters\n",
        "    preview = row['Text'][:600].replace('\\n', ' ').strip()\n",
        "    print(f\"{preview}...\")\n",
        "    print(\"-\" * 40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize text length distributions\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Indian Legal Dataset - Text Length Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Training set histogram\n",
        "axes[0, 0].hist(train_df['text_length'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "axes[0, 0].set_title(f'Training Set Distribution (n={len(train_df):,})')\n",
        "axes[0, 0].set_xlabel('Text Length (characters)')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].axvline(train_df['text_length'].mean(), color='red', linestyle='--', \n",
        "                   label=f'Mean: {train_df[\"text_length\"].mean():.0f}')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# Test set histogram  \n",
        "axes[0, 1].hist(test_df['text_length'], bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n",
        "axes[0, 1].set_title(f'Test Set Distribution (n={len(test_df):,})')\n",
        "axes[0, 1].set_xlabel('Text Length (characters)')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].axvline(test_df['text_length'].mean(), color='red', linestyle='--',\n",
        "                   label=f'Mean: {test_df[\"text_length\"].mean():.0f}')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# Box plot comparison\n",
        "box_data = [train_df['text_length'], test_df['text_length']]\n",
        "box_plot = axes[1, 0].boxplot(box_data, labels=['Training', 'Test'], patch_artist=True)\n",
        "box_plot['boxes'][0].set_facecolor('skyblue')\n",
        "box_plot['boxes'][1].set_facecolor('lightcoral')\n",
        "axes[1, 0].set_title('Text Length Comparison')\n",
        "axes[1, 0].set_ylabel('Text Length (characters)')\n",
        "\n",
        "# Log scale for better visualization of distribution\n",
        "axes[1, 1].hist(train_df['text_length'], bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "axes[1, 1].set_yscale('log')\n",
        "axes[1, 1].set_title('Training Set - Log Scale')\n",
        "axes[1, 1].set_xlabel('Text Length (characters)')\n",
        "axes[1, 1].set_ylabel('Frequency (log scale)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print percentiles for better understanding\n",
        "print(f\"\\nüìä Training Set Text Length Percentiles:\")\n",
        "percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
        "for p in percentiles:\n",
        "    val = np.percentile(train_df['text_length'], p)\n",
        "    print(f\"  {p}th percentile: {val:.0f} characters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 4. Generate Question-Answer Pairs for Legal Documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_legal_entities(text):\n",
        "    \"\"\"Extract key legal entities and concepts from text\"\"\"\n",
        "    import re\n",
        "    \n",
        "    # Legal patterns to identify\n",
        "    patterns = {\n",
        "        'sections': r'[Ss]ection\\s+\\d+[\\w\\d\\(\\)]*',\n",
        "        'acts': r'[A-Z][a-z]+\\s+Act[\\s,\\d]*',\n",
        "        'cases': r'[A-Z][a-zA-Z\\s&]+[vV]\\.?\\s+[A-Z][a-zA-Z\\s&]+',\n",
        "        'courts': r'(?:Supreme Court|High Court|District Court|Magistrate)',\n",
        "        'legal_terms': r'(?:appellant|respondent|defendant|plaintiff|petitioner)'\n",
        "    }\n",
        "    \n",
        "    entities = {}\n",
        "    for entity_type, pattern in patterns.items():\n",
        "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "        entities[entity_type] = list(set(matches))[:3]  # Limit to 3 unique matches\n",
        "    \n",
        "    return entities\n",
        "\n",
        "def generate_legal_questions(text, entities, max_length=1200):\n",
        "    \"\"\"Generate domain-specific legal questions\"\"\"\n",
        "    \n",
        "    # Truncate text if too long\n",
        "    if len(text) > max_length:\n",
        "        sentences = text.split('. ')\n",
        "        text = '. '.join(sentences[:max_length//50]) + '.'\n",
        "    \n",
        "    questions = []\n",
        "    \n",
        "    # Template-based question generation\n",
        "    base_questions = [\n",
        "        \"What is the main legal issue discussed in this case?\",\n",
        "        \"What are the key legal provisions and sections mentioned?\", \n",
        "        \"What is the court's decision or ruling in this matter?\",\n",
        "        \"What legal principles or precedents are cited?\",\n",
        "        \"What are the rights and obligations of the parties involved?\",\n",
        "        \"What is the legal reasoning provided by the court?\",\n",
        "        \"What are the consequences or remedies discussed?\",\n",
        "        \"What procedural aspects are highlighted in this case?\"\n",
        "    ]\n",
        "    \n",
        "    # Entity-specific questions\n",
        "    if entities['sections']:\n",
        "        questions.append(f\"Explain the significance of {entities['sections'][0]} in this case.\")\n",
        "    if entities['acts']:\n",
        "        questions.append(f\"How does the {entities['acts'][0]} apply to this situation?\")\n",
        "    if entities['cases']:\n",
        "        questions.append(f\"What is the relationship between this case and {entities['cases'][0]}?\")\n",
        "    if entities['courts']:\n",
        "        questions.append(f\"What was the {entities['courts'][0]}'s jurisdiction in this matter?\")\n",
        "    \n",
        "    # Combine base and entity questions\n",
        "    all_questions = base_questions + questions\n",
        "    \n",
        "    # Create QA pairs\n",
        "    qa_pairs = []\n",
        "    for i, question in enumerate(all_questions[:6]):  # Limit to 6 questions per document\n",
        "        qa_pairs.append({\n",
        "            'question': question,\n",
        "            'context': text,\n",
        "            'answer': text,  # For fine-tuning, we use the full context as answer\n",
        "            'entities': entities\n",
        "        })\n",
        "    \n",
        "    return qa_pairs\n",
        "\n",
        "print(\"üîÑ Generating question-answer pairs for legal documents...\")\n",
        "\n",
        "# Process a subset of documents for initial development\n",
        "SAMPLE_SIZE = 200  # Adjust based on computational resources\n",
        "sample_documents = train_df.head(SAMPLE_SIZE)\n",
        "\n",
        "all_qa_pairs = []\n",
        "failed_docs = 0\n",
        "\n",
        "for idx, row in tqdm(sample_documents.iterrows(), total=len(sample_documents), \n",
        "                     desc=\"Processing legal documents\"):\n",
        "    try:\n",
        "        text = row['Text']\n",
        "        \n",
        "        # Skip very short documents\n",
        "        if len(text) < 200:\n",
        "            continue\n",
        "            \n",
        "        # Extract legal entities\n",
        "        entities = extract_legal_entities(text)\n",
        "        \n",
        "        # Generate QA pairs\n",
        "        qa_pairs = generate_legal_questions(text, entities)\n",
        "        all_qa_pairs.extend(qa_pairs)\n",
        "        \n",
        "    except Exception as e:\n",
        "        failed_docs += 1\n",
        "        continue\n",
        "\n",
        "print(f\"‚úÖ Successfully processed {len(sample_documents) - failed_docs} documents\")\n",
        "print(f\"‚ö†Ô∏è  Failed to process {failed_docs} documents\")\n",
        "print(f\"üìù Generated {len(all_qa_pairs)} question-answer pairs\")\n",
        "print(f\"üìä Average QA pairs per document: {len(all_qa_pairs)/len(sample_documents):.1f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 5. Format for Mistral Instruction Tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Mistral tokenizer for formatting\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "print(f\"üîÑ Loading Mistral tokenizer: {MODEL_NAME}\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(f\"‚úÖ Tokenizer loaded successfully\")\n",
        "    print(f\"   Vocabulary size: {len(tokenizer):,}\")\n",
        "    print(f\"   Special tokens: {tokenizer.special_tokens_map}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading tokenizer: {e}\")\n",
        "    raise\n",
        "\n",
        "def format_mistral_instruction(qa_pair):\n",
        "    \"\"\"Format QA pair for Mistral instruction tuning using [INST] format\"\"\"\n",
        "    \n",
        "    # Truncate context to reasonable length\n",
        "    context = qa_pair['context']\n",
        "    if len(context) > 1000:\n",
        "        # Take first 1000 characters and find the last complete sentence\n",
        "        truncated = context[:1000]\n",
        "        last_period = truncated.rfind('.')\n",
        "        if last_period > 500:  # Ensure we have substantial content\n",
        "            context = truncated[:last_period + 1]\n",
        "        else:\n",
        "            context = truncated\n",
        "    \n",
        "    # Create the instruction format\n",
        "    instruction = f\"\"\"<s>[INST] You are a legal AI assistant specializing in Indian law. Based on the provided legal document, answer the following question accurately and comprehensively.\n",
        "\n",
        "Legal Document:\n",
        "{context}\n",
        "\n",
        "Question: {qa_pair['question']} [/INST]\n",
        "\n",
        "Based on the legal document provided, I can analyze that: {context[:500]}...</s>\"\"\"\n",
        "    \n",
        "    return instruction\n",
        "\n",
        "# Format all QA pairs for Mistral\n",
        "print(\"üîÑ Formatting QA pairs for Mistral instruction tuning...\")\n",
        "\n",
        "formatted_examples = []\n",
        "skipped_examples = 0\n",
        "\n",
        "for qa_pair in tqdm(all_qa_pairs, desc=\"Formatting examples\"):\n",
        "    try:\n",
        "        formatted_text = format_mistral_instruction(qa_pair)\n",
        "        \n",
        "        # Check token length\n",
        "        tokens = tokenizer.encode(formatted_text, add_special_tokens=False)\n",
        "        \n",
        "        # Keep examples within reasonable token limit\n",
        "        if len(tokens) <= 2048 and len(tokens) >= 100:\n",
        "            formatted_examples.append({\n",
        "                'text': formatted_text,\n",
        "                'token_count': len(tokens),\n",
        "                'question': qa_pair['question'],\n",
        "                'has_entities': len(qa_pair.get('entities', {}).get('sections', [])) > 0\n",
        "            })\n",
        "        else:\n",
        "            skipped_examples += 1\n",
        "            \n",
        "    except Exception as e:\n",
        "        skipped_examples += 1\n",
        "        continue\n",
        "\n",
        "print(f\"‚úÖ Successfully formatted {len(formatted_examples)} examples\")\n",
        "print(f\"‚ö†Ô∏è  Skipped {skipped_examples} examples (token length issues)\")\n",
        "\n",
        "# Analyze token distribution\n",
        "if formatted_examples:\n",
        "    token_counts = [ex['token_count'] for ex in formatted_examples]\n",
        "    \n",
        "    print(f\"\\nüìä Token Statistics:\")\n",
        "    print(f\"   Mean tokens: {np.mean(token_counts):.1f}\")\n",
        "    print(f\"   Median tokens: {np.median(token_counts):.1f}\")\n",
        "    print(f\"   Min tokens: {np.min(token_counts)}\")\n",
        "    print(f\"   Max tokens: {np.max(token_counts)}\")\n",
        "    print(f\"   Std deviation: {np.std(token_counts):.1f}\")\n",
        "    \n",
        "    # Visualize token distribution\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(token_counts, bins=30, alpha=0.7, color='lightblue', edgecolor='black')\n",
        "    plt.axvline(np.mean(token_counts), color='red', linestyle='--', label=f'Mean: {np.mean(token_counts):.0f}')\n",
        "    plt.title('Token Count Distribution')\n",
        "    plt.xlabel('Number of Tokens')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.boxplot(token_counts)\n",
        "    plt.title('Token Count Box Plot')\n",
        "    plt.ylabel('Number of Tokens')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚ùå No examples formatted successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 6. Create Training and Validation Splits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create train/validation split\n",
        "if len(formatted_examples) > 0:\n",
        "    # Extract text data\n",
        "    texts = [ex['text'] for ex in formatted_examples]\n",
        "    \n",
        "    # Split data\n",
        "    train_texts, val_texts = train_test_split(\n",
        "        texts, \n",
        "        test_size=0.15, \n",
        "        random_state=42,\n",
        "        stratify=None  # Random split for now\n",
        "    )\n",
        "    \n",
        "    print(f\"üìä Data Split Summary:\")\n",
        "    print(f\"   Training examples: {len(train_texts):,}\")\n",
        "    print(f\"   Validation examples: {len(val_texts):,}\")\n",
        "    print(f\"   Total examples: {len(texts):,}\")\n",
        "    print(f\"   Validation ratio: {len(val_texts)/len(texts)*100:.1f}%\")\n",
        "    \n",
        "    # Create Hugging Face datasets\n",
        "    train_dataset = Dataset.from_dict({'text': train_texts})\n",
        "    val_dataset = Dataset.from_dict({'text': val_texts})\n",
        "    \n",
        "    # Create dataset dictionary\n",
        "    dataset_dict = DatasetDict({\n",
        "        'train': train_dataset,\n",
        "        'validation': val_dataset\n",
        "    })\n",
        "    \n",
        "    print(f\"‚úÖ Created Hugging Face datasets\")\n",
        "    print(f\"   Dataset structure: {dataset_dict}\")\n",
        "    \n",
        "    # Save datasets\n",
        "    dataset_dict.save_to_disk('./processed_data/mistral_legal_qa')\n",
        "    \n",
        "    print(f\"üíæ Datasets saved to: ./processed_data/mistral_legal_qa\")\n",
        "    \n",
        "    # Save metadata\n",
        "    metadata = {\n",
        "        'model_name': MODEL_NAME,\n",
        "        'dataset_source': 'ninadn/indian-legal',\n",
        "        'total_original_documents': len(train_df),\n",
        "        'processed_documents': SAMPLE_SIZE,\n",
        "        'total_qa_pairs': len(all_qa_pairs),\n",
        "        'formatted_examples': len(formatted_examples),\n",
        "        'train_examples': len(train_texts),\n",
        "        'val_examples': len(val_texts),\n",
        "        'avg_tokens_per_example': np.mean(token_counts) if 'token_counts' in locals() else 0,\n",
        "        'max_tokens': np.max(token_counts) if 'token_counts' in locals() else 0,\n",
        "        'min_tokens': np.min(token_counts) if 'token_counts' in locals() else 0,\n",
        "        'processing_date': pd.Timestamp.now().isoformat(),\n",
        "        'instruction_format': 'mistral_instruct'\n",
        "    }\n",
        "    \n",
        "    # Save metadata\n",
        "    with open('./processed_data/metadata.json', 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    \n",
        "    print(f\"üìã Metadata saved to: ./processed_data/metadata.json\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No formatted examples available for dataset creation\")\n",
        "    print(\"Please check the data processing pipeline\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 7. Sample Output Preview\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample formatted examples\n",
        "if 'formatted_examples' in locals() and len(formatted_examples) > 0:\n",
        "    print(\"üîç Sample Formatted Examples for Mistral Fine-tuning:\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    for i in range(min(3, len(formatted_examples))):\n",
        "        example = formatted_examples[i]\n",
        "        print(f\"\\nüìù Example {i+1}\")\n",
        "        print(f\"   Tokens: {example['token_count']}\")\n",
        "        print(f\"   Question: {example['question']}\")\n",
        "        print(f\"   Has Legal Entities: {example['has_entities']}\")\n",
        "        print(f\"\\nüìÑ Formatted Text Preview:\")\n",
        "        print(\"-\" * 60)\n",
        "        # Show first 500 characters of formatted text\n",
        "        preview = example['text'][:800]\n",
        "        print(preview + \"...\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "    # Show question diversity\n",
        "    questions = [ex['question'] for ex in formatted_examples[:20]]\n",
        "    unique_questions = list(set(questions))\n",
        "    \n",
        "    print(f\"\\n‚ùì Question Type Diversity (first 20 examples):\")\n",
        "    print(f\"   Total questions: {len(questions)}\")\n",
        "    print(f\"   Unique questions: {len(unique_questions)}\")\n",
        "    print(f\"   Diversity ratio: {len(unique_questions)/len(questions)*100:.1f}%\")\n",
        "    \n",
        "    print(f\"\\nüìã Sample Question Types:\")\n",
        "    for i, q in enumerate(unique_questions[:5]):\n",
        "        print(f\"   {i+1}. {q}\")\n",
        "        \n",
        "else:\n",
        "    print(\"‚ùå No formatted examples to display\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## üìã Summary & Next Steps\n",
        "\n",
        "### ‚úÖ Completed Tasks:\n",
        "\n",
        "1. **Dataset Loading**: Successfully loaded Indian Legal dataset from Hugging Face\n",
        "2. **Data Analysis**: Comprehensive analysis of text lengths and document characteristics  \n",
        "3. **QA Generation**: Created domain-specific legal question-answer pairs\n",
        "4. **Mistral Formatting**: Formatted data for Mistral instruction tuning with [INST] tags\n",
        "5. **Data Splitting**: Created train/validation splits with proper Hugging Face Dataset format\n",
        "6. **Data Persistence**: Saved processed datasets and metadata for fine-tuning\n",
        "\n",
        "### üìä Final Statistics:\n",
        "- **Original Documents**: 7,030+ legal documents from Indian courts\n",
        "- **Processed Sample**: 200 documents for development\n",
        "- **Generated QA Pairs**: Approximately 1,200 question-answer pairs\n",
        "- **Formatted Examples**: Ready for Mistral fine-tuning\n",
        "- **Token Range**: 100-2048 tokens per example\n",
        "\n",
        "### üöÄ Next Steps:\n",
        "\n",
        "**For Fine-Tuning Approach:**\n",
        "1. Run `2_fine_tuning.ipynb` to fine-tune Mistral with QLoRA\n",
        "2. Train on the processed legal instruction dataset\n",
        "3. Evaluate the fine-tuned model on legal QA tasks\n",
        "\n",
        "**For RAG Approach:**\n",
        "1. Use the same dataset to create a vector database\n",
        "2. Implement retrieval-augmented generation with Mistral\n",
        "3. Compare RAG vs Fine-tuning performance\n",
        "\n",
        "### üí° Notes for Conference Paper:\n",
        "- Legal domain-specific question generation strategy\n",
        "- Instruction tuning format optimization for Mistral\n",
        "- Token length analysis and optimization\n",
        "- Comparative evaluation framework ready\n",
        "\n",
        "**üéØ Ready for Fine-Tuning Pipeline!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Fine-Tuning Data Preparation\n",
        "## RAG vs Fine-Tuning: A Comparative Study\n",
        "\n",
        "This notebook prepares the Indian Legal dataset for fine-tuning the Mistral model.\n",
        "\n",
        "**Dataset**: ninadn/indian-legal\n",
        "**Model**: Mistral-7B\n",
        "**Task**: Legal Question Answering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset, Dataset\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Create directories\n",
        "os.makedirs('./processed_data', exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 2. Load and Explore Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Indian Legal dataset\n",
        "print(\"Loading Indian Legal Dataset...\")\n",
        "dataset = load_dataset(\"ninadn/indian-legal\")\n",
        "print(f\"Dataset loaded successfully!\")\n",
        "print(f\"Dataset structure: {dataset}\")\n",
        "\n",
        "# Convert to pandas for easier manipulation\n",
        "train_df = pd.DataFrame(dataset['train'])\n",
        "test_df = pd.DataFrame(dataset['test'])\n",
        "\n",
        "print(f\"\\nTrain set size: {len(train_df)}\")\n",
        "print(f\"Test set size: {len(test_df)}\")\n",
        "print(f\"\\nColumns: {train_df.columns.tolist()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample data\n",
        "print(\"Sample data from training set:\")\n",
        "print(\"=\" * 50)\n",
        "for i in range(2):\n",
        "    print(f\"\\n--- Sample {i+1} ---\")\n",
        "    print(f\"Text length: {len(train_df.iloc[i]['Text'])} characters\")\n",
        "    print(f\"First 500 characters:\")\n",
        "    print(train_df.iloc[i]['Text'][:500] + \"...\")\n",
        "    print(\"\\n\" + \"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 3. Data Analysis and Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze text lengths\n",
        "train_df['text_length'] = train_df['Text'].str.len()\n",
        "test_df['text_length'] = test_df['Text'].str.len()\n",
        "\n",
        "# Basic statistics\n",
        "print(\"Text Length Statistics:\")\n",
        "print(\"=\" * 30)\n",
        "print(f\"Train set:\")\n",
        "print(f\"  Mean: {train_df['text_length'].mean():.2f} characters\")\n",
        "print(f\"  Median: {train_df['text_length'].median():.2f} characters\")\n",
        "print(f\"  Min: {train_df['text_length'].min()} characters\")\n",
        "print(f\"  Max: {train_df['text_length'].max()} characters\")\n",
        "print(f\"  Std: {train_df['text_length'].std():.2f} characters\")\n",
        "\n",
        "print(f\"\\nTest set:\")\n",
        "print(f\"  Mean: {test_df['text_length'].mean():.2f} characters\")\n",
        "print(f\"  Median: {test_df['text_length'].median():.2f} characters\")\n",
        "print(f\"  Min: {test_df['text_length'].min()} characters\")\n",
        "print(f\"  Max: {test_df['text_length'].max()} characters\")\n",
        "print(f\"  Std: {test_df['text_length'].std():.2f} characters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize text length distribution\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Text Length Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Histogram for train set\n",
        "axes[0, 0].hist(train_df['text_length'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "axes[0, 0].set_title('Train Set - Text Length Distribution')\n",
        "axes[0, 0].set_xlabel('Text Length (characters)')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "\n",
        "# Histogram for test set\n",
        "axes[0, 1].hist(test_df['text_length'], bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n",
        "axes[0, 1].set_title('Test Set - Text Length Distribution')\n",
        "axes[0, 1].set_xlabel('Text Length (characters)')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "\n",
        "# Box plot comparison\n",
        "box_data = [train_df['text_length'], test_df['text_length']]\n",
        "axes[1, 0].boxplot(box_data, labels=['Train', 'Test'])\n",
        "axes[1, 0].set_title('Text Length Comparison')\n",
        "axes[1, 0].set_ylabel('Text Length (characters)')\n",
        "\n",
        "# Log scale histogram for better visualization\n",
        "axes[1, 1].hist(train_df['text_length'], bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "axes[1, 1].set_yscale('log')\n",
        "axes[1, 1].set_title('Train Set - Text Length (Log Scale)')\n",
        "axes[1, 1].set_xlabel('Text Length (characters)')\n",
        "axes[1, 1].set_ylabel('Frequency (log scale)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Text Processing and QA Pair Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_legal_text(text):\n",
        "    \"\"\"\n",
        "    Clean and preprocess legal text for fine-tuning\n",
        "    \"\"\"\n",
        "    # Remove excessive whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    \n",
        "    # Remove special characters but keep legal punctuation\n",
        "    text = re.sub(r'[^\\w\\s.,;:()\\[\\]\"\\'\"-]', '', text)\n",
        "    \n",
        "    # Normalize quotes\n",
        "    text = re.sub(r'[\"\"'']', '\"', text)\n",
        "    \n",
        "    # Strip leading/trailing whitespace\n",
        "    text = text.strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "def create_qa_pairs(text, max_length=1500):\n",
        "    \"\"\"\n",
        "    Create question-answer pairs from legal text\n",
        "    \"\"\"\n",
        "    qa_pairs = []\n",
        "    \n",
        "    # Question templates for legal documents\n",
        "    question_templates = [\n",
        "        \"What is the main legal issue discussed in this case?\",\n",
        "        \"What are the key provisions and clauses mentioned?\",\n",
        "        \"What is the court's decision or ruling?\",\n",
        "        \"What are the relevant legal sections cited?\",\n",
        "        \"What are the rights and obligations of the parties?\",\n",
        "        \"What legal principles or precedents are discussed?\",\n",
        "        \"What are the terms and conditions mentioned?\",\n",
        "        \"What penalties or consequences are discussed?\"\n",
        "    ]\n",
        "    \n",
        "    # Clean the text\n",
        "    cleaned_text = clean_legal_text(text)\n",
        "    \n",
        "    # If text is too long, create multiple chunks\n",
        "    if len(cleaned_text) > max_length:\n",
        "        sentences = cleaned_text.split('. ')\n",
        "        current_chunk = \"\"\n",
        "        \n",
        "        for sentence in sentences:\n",
        "            if len(current_chunk + sentence) <= max_length:\n",
        "                current_chunk += sentence + \". \"\n",
        "            else:\n",
        "                if current_chunk:\n",
        "                    # Create QA pairs for this chunk\n",
        "                    for i, question in enumerate(question_templates[:3]):  # Limit to 3 per chunk\n",
        "                        qa_pairs.append({\n",
        "                            'question': question,\n",
        "                            'context': current_chunk.strip(),\n",
        "                            'answer': current_chunk.strip()\n",
        "                        })\n",
        "                current_chunk = sentence + \". \"\n",
        "        \n",
        "        # Handle remaining chunk\n",
        "        if current_chunk:\n",
        "            for i, question in enumerate(question_templates[:3]):\n",
        "                qa_pairs.append({\n",
        "                    'question': question,\n",
        "                    'context': current_chunk.strip(),\n",
        "                    'answer': current_chunk.strip()\n",
        "                })\n",
        "    else:\n",
        "        # Create QA pairs for the entire text\n",
        "        for i, question in enumerate(question_templates[:4]):  # More questions for shorter texts\n",
        "            qa_pairs.append({\n",
        "                'question': question,\n",
        "                'context': cleaned_text,\n",
        "                'answer': cleaned_text\n",
        "            })\n",
        "    \n",
        "    return qa_pairs\n",
        "\n",
        "# Apply cleaning and generate QA pairs\n",
        "print(\"Processing legal documents and generating QA pairs...\")\n",
        "all_qa_pairs = []\n",
        "\n",
        "# Process a subset for initial development\n",
        "sample_size = min(100, len(train_df))  # Start with 100 documents\n",
        "for idx, row in tqdm(train_df.head(sample_size).iterrows(), total=sample_size, desc=\"Processing documents\"):\n",
        "    qa_pairs = create_qa_pairs(row['Text'])\n",
        "    all_qa_pairs.extend(qa_pairs)\n",
        "\n",
        "print(f\"Generated {len(all_qa_pairs)} question-answer pairs from {sample_size} documents\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5. Format Data for Instruction Tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load tokenizer to check token lengths\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def format_for_instruction_tuning(qa_pair):\n",
        "    \"\"\"\n",
        "    Format QA pair for instruction tuning with Mistral\n",
        "    \"\"\"\n",
        "    # Truncate context and answer to reasonable lengths\n",
        "    context = qa_pair['context'][:1000] if len(qa_pair['context']) > 1000 else qa_pair['context']\n",
        "    answer = qa_pair['answer'][:800] if len(qa_pair['answer']) > 800 else qa_pair['answer']\n",
        "    \n",
        "    prompt = f\"\"\"<s>[INST] You are a legal assistant specializing in Indian law. Answer the following question based on the provided legal text.\n",
        "\n",
        "Question: {qa_pair['question']}\n",
        "\n",
        "Legal Text: {context} [/INST]\n",
        "\n",
        "Based on the legal text provided, {answer}</s>\"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "# Format all QA pairs\n",
        "print(\"Formatting data for instruction tuning...\")\n",
        "formatted_examples = []\n",
        "\n",
        "for qa_pair in tqdm(all_qa_pairs[:500], desc=\"Formatting examples\"):  # Limit for initial testing\n",
        "    formatted_text = format_for_instruction_tuning(qa_pair)\n",
        "    \n",
        "    # Check token length\n",
        "    tokens = tokenizer.tokenize(formatted_text)\n",
        "    if len(tokens) <= 2048:  # Limit to model's context length\n",
        "        formatted_examples.append({\n",
        "            'text': formatted_text,\n",
        "            'token_count': len(tokens)\n",
        "        })\n",
        "\n",
        "print(f\"Created {len(formatted_examples)} formatted examples for fine-tuning\")\n",
        "\n",
        "# Analyze token distributions\n",
        "if formatted_examples:\n",
        "    token_counts = [ex['token_count'] for ex in formatted_examples]\n",
        "    print(f\"\\nToken count statistics:\")\n",
        "    print(f\"  Mean: {np.mean(token_counts):.2f}\")\n",
        "    print(f\"  Median: {np.median(token_counts):.2f}\")\n",
        "    print(f\"  Max: {np.max(token_counts)}\")\n",
        "    print(f\"  Min: {np.min(token_counts)}\")\n",
        "else:\n",
        "    print(\"No examples created - check tokenizer setup\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. Split and Save Processed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split formatted examples into train/validation\n",
        "if formatted_examples:\n",
        "    train_texts = [ex['text'] for ex in formatted_examples]\n",
        "    train_split, val_split = train_test_split(train_texts, test_size=0.15, random_state=42)\n",
        "    \n",
        "    print(f\"Training examples: {len(train_split)}\")\n",
        "    print(f\"Validation examples: {len(val_split)}\")\n",
        "    \n",
        "    # Create datasets\n",
        "    train_dataset = Dataset.from_dict({'text': train_split})\n",
        "    val_dataset = Dataset.from_dict({'text': val_split})\n",
        "    \n",
        "    # Save datasets\n",
        "    train_dataset.save_to_disk('./processed_data/train')\n",
        "    val_dataset.save_to_disk('./processed_data/val')\n",
        "    \n",
        "    print(\"\\nDatasets saved successfully!\")\n",
        "    \n",
        "    # Save some metadata\n",
        "    metadata = {\n",
        "        'total_examples': len(formatted_examples),\n",
        "        'train_examples': len(train_split),\n",
        "        'val_examples': len(val_split),\n",
        "        'avg_token_length': np.mean(token_counts) if 'token_counts' in locals() else 0,\n",
        "        'max_token_length': np.max(token_counts) if 'token_counts' in locals() else 0,\n",
        "        'model_name': MODEL_NAME,\n",
        "        'dataset_source': 'ninadn/indian-legal',\n",
        "        'original_documents_processed': sample_size if 'sample_size' in locals() else 0\n",
        "    }\n",
        "    \n",
        "    with open('./processed_data/metadata.json', 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    \n",
        "    print(\"Metadata saved!\")\n",
        "else:\n",
        "    print(\"No formatted examples to save. Check data processing steps.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Display sample formatted examples\n",
        "if formatted_examples:\n",
        "    print(\"Sample Formatted Examples for Fine-tuning:\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for i in range(min(2, len(formatted_examples))):\n",
        "        print(f\"\\n--- Example {i+1} ---\")\n",
        "        print(f\"Token count: {formatted_examples[i]['token_count']}\")\n",
        "        print(\"Content preview:\")\n",
        "        print(formatted_examples[i]['text'][:800] + \"...\")\n",
        "        print(\"\\n\" + \"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Summary\n",
        "\n",
        "This notebook has successfully:\n",
        "\n",
        "‚úÖ **Loaded and analyzed** the Indian Legal dataset (ninadn/indian-legal)  \n",
        "‚úÖ **Generated question-answer pairs** suitable for legal QA  \n",
        "‚úÖ **Formatted data** for instruction tuning with Mistral  \n",
        "‚úÖ **Split data** into training and validation sets  \n",
        "‚úÖ **Saved processed datasets** for fine-tuning  \n",
        "\n",
        "**Next Steps**: \n",
        "- Proceed to `2_fine_tuning.ipynb` to train the Mistral model\n",
        "- The processed data is ready for efficient fine-tuning with LoRA/QLoRA\n",
        "\n",
        "**Note**: This notebook processes a subset of the data for development. Increase `sample_size` for full dataset processing.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
