{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Fine-Tuning Mistral-7B for Legal QA\n",
        "## RAG vs Fine-Tuning: A Comparative Study\n",
        "\n",
        "This notebook fine-tunes the Mistral-7B model on the processed Indian Legal dataset using QLoRA (Quantized LoRA) for efficient training.\n",
        "\n",
        "**Key Features:**\n",
        "- QLoRA for memory-efficient training\n",
        "- Legal domain-specific instruction tuning  \n",
        "- Comprehensive monitoring and evaluation\n",
        "- Model saving and deployment preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_from_disk\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    TaskType\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check GPU availability\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "else:\n",
        "    print(\"Running on CPU - training will be slower\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 2. Load Processed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the processed datasets\n",
        "try:\n",
        "    train_dataset = load_from_disk('./processed_data/train')\n",
        "    val_dataset = load_from_disk('./processed_data/val')\n",
        "    \n",
        "    print(f\"‚úÖ Training dataset loaded: {len(train_dataset)} examples\")\n",
        "    print(f\"‚úÖ Validation dataset loaded: {len(val_dataset)} examples\")\n",
        "    \n",
        "    # Load metadata\n",
        "    with open('./processed_data/metadata.json', 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "    \n",
        "    print(f\"\\nüìä Dataset Statistics:\")\n",
        "    for key, value in metadata.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "        \n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Processed data not found. Please run 1_data_preparation.ipynb first.\")\n",
        "    raise\n",
        "\n",
        "# Display sample data\n",
        "print(f\"\\nüìù Sample Training Example:\")\n",
        "print(\"=\" * 60)\n",
        "print(train_dataset[0]['text'][:800] + \"...\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 3. Model and Tokenizer Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
        "OUTPUT_DIR = \"./fine_tuned_legal_mistral\"\n",
        "\n",
        "# QLoRA configuration for efficient training\n",
        "qlora_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "print(f\"üîÑ Loading model: {MODEL_NAME}\")\n",
        "print(\"Using QLoRA (4-bit quantization) for memory efficiency...\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(f\"‚úÖ Tokenizer loaded\")\n",
        "print(f\"  Vocab size: {len(tokenizer)}\")\n",
        "print(f\"  Pad token: {tokenizer.pad_token}\")\n",
        "\n",
        "# Load model with quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=qlora_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model loaded with 4-bit quantization\")\n",
        "print(f\"  Model device: {next(model.parameters()).device}\")\n",
        "print(f\"  Model dtype: {next(model.parameters()).dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. LoRA Configuration and Model Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA configuration for Mistral\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # Rank\n",
        "    lora_alpha=32,  # Alpha parameter for LoRA scaling\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
        "                   \"gate_proj\", \"up_proj\", \"down_proj\"],  # Mistral attention layers\n",
        "    lora_dropout=0.05,  # Dropout probability for LoRA layers\n",
        "    bias=\"none\",  # Bias type\n",
        "    task_type=TaskType.CAUSAL_LM,  # Task type\n",
        ")\n",
        "\n",
        "print(\"üîß LoRA Configuration:\")\n",
        "print(f\"  Rank (r): {lora_config.r}\")\n",
        "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
        "print(f\"  Target modules: {lora_config.target_modules}\")\n",
        "print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
        "\n",
        "# Get PEFT model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable_params = 0\n",
        "all_param = 0\n",
        "for _, param in model.named_parameters():\n",
        "    all_param += param.numel()\n",
        "    if param.requires_grad:\n",
        "        trainable_params += param.numel()\n",
        "\n",
        "print(f\"\\nüìä Model Parameters:\")\n",
        "print(f\"  Trainable params: {trainable_params:,}\")\n",
        "print(f\"  All params: {all_param:,}\")\n",
        "print(f\"  Trainable%: {100 * trainable_params / all_param:.2f}%\")\n",
        "\n",
        "# Enable gradient checkpointing for memory efficiency\n",
        "model.gradient_checkpointing_enable()\n",
        "model.config.use_cache = False  # Disable cache for training\n",
        "\n",
        "print(f\"\\n‚úÖ Model prepared for training with LoRA\")\n",
        "print(f\"  Gradient checkpointing: Enabled\")\n",
        "print(f\"  Cache: Disabled for training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5. Training Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data collator for language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # We're doing causal language modeling, not masked\n",
        "    pad_to_multiple_of=8,  # For efficiency on modern hardware\n",
        ")\n",
        "\n",
        "# Training arguments for QLoRA fine-tuning\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=1,  # Small batch size for memory efficiency\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=8,  # Effective batch size = 1 * 8 = 8\n",
        "    num_train_epochs=3,  # Start with 3 epochs\n",
        "    learning_rate=2e-4,  # Higher learning rate for LoRA\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    eval_steps=50,\n",
        "    save_steps=100,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    fp16=True,  # Use mixed precision for efficiency\n",
        "    dataloader_pin_memory=False,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=None,  # Disable wandb/tensorboard for now\n",
        "    run_name=\"mistral-legal-qa-lora\",\n",
        "    seed=42,\n",
        "    data_seed=42,\n",
        "    optim=\"adamw_torch\",\n",
        "    max_grad_norm=1.0,\n",
        "    group_by_length=True,  # Group similar length sequences for efficiency\n",
        ")\n",
        "\n",
        "print(\"üîß Training Configuration:\")\n",
        "print(f\"  Output directory: {training_args.output_dir}\")\n",
        "print(f\"  Batch size (per device): {training_args.per_device_train_batch_size}\")\n",
        "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"  Number of epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"  Scheduler: {training_args.lr_scheduler_type}\")\n",
        "print(f\"  Mixed precision: {training_args.fp16}\")\n",
        "print(f\"  Optimizer: {training_args.optim}\")\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"‚úÖ Training configuration ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. Data Preprocessing for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize the training examples\"\"\"\n",
        "    # Tokenize the text\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=False,  # We'll pad dynamically\n",
        "        max_length=2048,\n",
        "        return_tensors=None,\n",
        "    )\n",
        "    \n",
        "    # For causal language modeling, labels are the same as input_ids\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    \n",
        "    return tokenized\n",
        "\n",
        "print(\"üîÑ Tokenizing datasets...\")\n",
        "\n",
        "# Tokenize the datasets\n",
        "tokenized_train = train_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names,\n",
        "    desc=\"Tokenizing training data\"\n",
        ")\n",
        "\n",
        "tokenized_val = val_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=val_dataset.column_names,\n",
        "    desc=\"Tokenizing validation data\"\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Tokenization complete\")\n",
        "print(f\"  Training examples: {len(tokenized_train)}\")\n",
        "print(f\"  Validation examples: {len(tokenized_val)}\")\n",
        "\n",
        "# Check a sample\n",
        "sample_tokens = tokenized_train[0]\n",
        "print(f\"\\nüìä Sample tokenized example:\")\n",
        "print(f\"  Input length: {len(sample_tokens['input_ids'])} tokens\")\n",
        "print(f\"  Labels length: {len(sample_tokens['labels'])} tokens\")\n",
        "print(f\"  First 10 tokens: {sample_tokens['input_ids'][:10]}\")\n",
        "\n",
        "# Check token length distribution\n",
        "train_lengths = [len(example['input_ids']) for example in tokenized_train]\n",
        "print(f\"\\nüìè Token length statistics:\")\n",
        "print(f\"  Mean: {np.mean(train_lengths):.1f}\")\n",
        "print(f\"  Max: {np.max(train_lengths)}\")\n",
        "print(f\"  Min: {np.min(train_lengths)}\")\n",
        "print(f\"  Std: {np.std(train_lengths):.1f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 7. Initialize Trainer and Start Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"üöÄ Trainer initialized successfully!\")\n",
        "print(f\"üìä Training overview:\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Training samples: {len(tokenized_train):,}\")\n",
        "print(f\"  Validation samples: {len(tokenized_val):,}\")\n",
        "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"  Total steps: {len(tokenized_train) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")\n",
        "\n",
        "# Check GPU memory before training\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\nüîß GPU Memory Status:\")\n",
        "    print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
        "    print(f\"  Reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
        "    print(f\"  Free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved(0)) / 1024**3:.2f} GB\")\n",
        "\n",
        "print(f\"\\n‚è∞ Starting training at {pd.Timestamp.now()}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "try:\n",
        "    print(\"üî• Training started...\")\n",
        "    training_output = trainer.train()\n",
        "    \n",
        "    print(f\"\\n‚úÖ Training completed successfully!\")\n",
        "    print(f\"üìä Training results:\")\n",
        "    print(f\"  Final training loss: {training_output.training_loss:.4f}\")\n",
        "    print(f\"  Training time: {training_output.metrics['train_runtime']:.2f} seconds\")\n",
        "    print(f\"  Samples per second: {training_output.metrics['train_samples_per_second']:.2f}\")\n",
        "    print(f\"  Steps per second: {training_output.metrics['train_steps_per_second']:.2f}\")\n",
        "    \n",
        "    # Get final evaluation\n",
        "    print(\"\\nüìà Running final evaluation...\")\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(f\"Final evaluation loss: {eval_results['eval_loss']:.4f}\")\n",
        "    print(f\"Perplexity: {np.exp(eval_results['eval_loss']):.2f}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Training failed with error: {e}\")\n",
        "    print(\"This might be due to memory constraints or other issues\")\n",
        "    print(\"Consider reducing batch size or sequence length\")\n",
        "    raise\n",
        "\n",
        "print(f\"\\n‚è∞ Training finished at {pd.Timestamp.now()}\")\n",
        "\n",
        "# Save the final model\n",
        "print(f\"\\nüíæ Saving fine-tuned model...\")\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(f\"‚úÖ Model saved to: {OUTPUT_DIR}\")\n",
        "print(f\"   - Model files: adapter_config.json, adapter_model.bin\")\n",
        "print(f\"   - Tokenizer files: tokenizer.json, tokenizer_config.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 8. Model Testing and Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the fine-tuned model\n",
        "def test_legal_qa(question, context, max_length=512):\n",
        "    \"\"\"Test the fine-tuned model on a legal QA task\"\"\"\n",
        "    \n",
        "    # Format the input like our training data\n",
        "    prompt = f\"\"\"<s>[INST] You are a legal AI assistant specializing in Indian law. Based on the provided legal document, answer the following question accurately and comprehensively.\n",
        "\n",
        "Legal Document:\n",
        "{context[:800]}\n",
        "\n",
        "Question: {question} [/INST]\n",
        "\n",
        "Based on the legal document provided, I can analyze that:\"\"\"\n",
        "    \n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1500)\n",
        "    \n",
        "    # Move to same device as model\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    \n",
        "    # Generate response\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_length,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "    \n",
        "    # Decode the response\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extract just the generated part (after [/INST])\n",
        "    if \"[/INST]\" in full_response:\n",
        "        response = full_response.split(\"[/INST]\")[-1].strip()\n",
        "    else:\n",
        "        response = full_response\n",
        "    \n",
        "    return response\n",
        "\n",
        "# Test with a sample from our validation set\n",
        "print(\"üß™ Testing the fine-tuned model...\")\n",
        "\n",
        "# Get a sample legal document\n",
        "sample_text = \"\"\"The Corporation made available to the Contractors different kinds of machinery and equipment for which the price paid by the Corporation inclusive of freight, insurance, customs duty etc. has to be charged to them. But the machinery and the equipment so made available to the Contractors were to remain the property of the Corporation until the full price thereof had been realised from the Contractors. There is a further condition that the Corporation will take over from the contractors items at their residual value calculated on the basis indicated in the agreement. The total approximate price is payable by the Contractors in 18 equal instalments.\"\"\"\n",
        "\n",
        "test_questions = [\n",
        "    \"What is the ownership arrangement for the machinery?\",\n",
        "    \"How is the payment structured for the machinery?\",\n",
        "    \"What happens to the machinery after full payment?\",\n",
        "    \"What are the key terms of this agreement?\"\n",
        "]\n",
        "\n",
        "print(\"üìù Sample Legal QA Responses:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, question in enumerate(test_questions):\n",
        "    print(f\"\\nüîç Question {i+1}: {question}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    try:\n",
        "        response = test_legal_qa(question, sample_text)\n",
        "        print(f\"ü§ñ Model Response:\")\n",
        "        print(response[:400] + \"...\" if len(response) > 400 else response)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generating response: {e}\")\n",
        "    \n",
        "    print(\"-\" * 60)\n",
        "\n",
        "print(f\"\\n‚úÖ Model testing completed!\")\n",
        "print(f\"üí° The model can now answer legal questions based on Indian legal documents\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 9. Save Training Metrics and Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save comprehensive training results for paper\n",
        "training_results = {\n",
        "    'model_name': MODEL_NAME,\n",
        "    'approach': 'fine_tuning',\n",
        "    'method': 'QLoRA',\n",
        "    'dataset': 'ninadn/indian-legal',\n",
        "    'training_params': {\n",
        "        'epochs': training_args.num_train_epochs,\n",
        "        'learning_rate': training_args.learning_rate,\n",
        "        'batch_size': training_args.per_device_train_batch_size,\n",
        "        'gradient_accumulation_steps': training_args.gradient_accumulation_steps,\n",
        "        'lora_r': lora_config.r,\n",
        "        'lora_alpha': lora_config.lora_alpha,\n",
        "        'lora_dropout': lora_config.lora_dropout,\n",
        "    },\n",
        "    'data_stats': {\n",
        "        'train_examples': len(tokenized_train),\n",
        "        'val_examples': len(tokenized_val),\n",
        "        'avg_tokens': np.mean(train_lengths) if 'train_lengths' in locals() else 0,\n",
        "        'max_tokens': np.max(train_lengths) if 'train_lengths' in locals() else 0,\n",
        "    },\n",
        "    'training_results': {\n",
        "        'final_train_loss': training_output.training_loss if 'training_output' in locals() else None,\n",
        "        'final_eval_loss': eval_results['eval_loss'] if 'eval_results' in locals() else None,\n",
        "        'perplexity': np.exp(eval_results['eval_loss']) if 'eval_results' in locals() else None,\n",
        "        'training_time': training_output.metrics['train_runtime'] if 'training_output' in locals() else None,\n",
        "        'samples_per_second': training_output.metrics['train_samples_per_second'] if 'training_output' in locals() else None,\n",
        "    },\n",
        "    'model_info': {\n",
        "        'trainable_parameters': trainable_params if 'trainable_params' in locals() else None,\n",
        "        'total_parameters': all_param if 'all_param' in locals() else None,\n",
        "        'trainable_percentage': (100 * trainable_params / all_param) if 'trainable_params' in locals() and 'all_param' in locals() else None,\n",
        "    },\n",
        "    'timestamp': pd.Timestamp.now().isoformat(),\n",
        "    'output_dir': OUTPUT_DIR\n",
        "}\n",
        "\n",
        "# Save results\n",
        "with open(f'{OUTPUT_DIR}/training_results.json', 'w') as f:\n",
        "    json.dump(training_results, f, indent=2)\n",
        "\n",
        "print(f\"üìä Training results saved to: {OUTPUT_DIR}/training_results.json\")\n",
        "\n",
        "# Display summary for conference paper\n",
        "print(f\"\\nüìã FINE-TUNING SUMMARY FOR CONFERENCE PAPER\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"üî¨ **Method**: QLoRA Fine-tuning of Mistral-7B\")\n",
        "print(f\"üìä **Dataset**: Indian Legal documents (ninadn/indian-legal)\")\n",
        "print(f\"üìù **Training Examples**: {len(tokenized_train):,}\")\n",
        "print(f\"üîß **Parameters**: {trainable_params:,} trainable ({(100 * trainable_params / all_param):.2f}% of total)\")\n",
        "print(f\"‚è±Ô∏è  **Training Time**: {training_output.metrics['train_runtime']:.1f} seconds\" if 'training_output' in locals() else \"‚è±Ô∏è  Training Time: Not recorded\")\n",
        "print(f\"üìâ **Final Loss**: {eval_results['eval_loss']:.4f}\" if 'eval_results' in locals() else \"üìâ Final Loss: Not recorded\")\n",
        "print(f\"üéØ **Perplexity**: {np.exp(eval_results['eval_loss']):.2f}\" if 'eval_results' in locals() else \"üéØ Perplexity: Not recorded\")\n",
        "print(f\"üíæ **Model Size**: LoRA adapters (~{(trainable_params * 4) / (1024**2):.1f} MB)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\n‚úÖ Fine-tuning pipeline completed successfully!\")\n",
        "print(f\"üéØ Ready for comparison with RAG approach\")\n",
        "print(f\"üìÅ All files saved in: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5. Training Configuration\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
