{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Fine-Tuning Mistral-7B for Legal QA\n",
        "## RAG vs Fine-Tuning: A Comparative Study\n",
        "\n",
        "This notebook fine-tunes the Mistral-7B model on the processed Indian Legal dataset using QLoRA (Quantized LoRA) for efficient training.\n",
        "\n",
        "**Key Features:**\n",
        "- QLoRA for memory-efficient training\n",
        "- Legal domain-specific instruction tuning  \n",
        "- Comprehensive monitoring and evaluation\n",
        "- Model saving and deployment preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_from_disk\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    TaskType\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check GPU availability\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "else:\n",
        "    print(\"Running on CPU - training will be slower\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 2. Load Processed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the processed datasets\n",
        "try:\n",
        "    train_dataset = load_from_disk('./processed_data/train')\n",
        "    val_dataset = load_from_disk('./processed_data/val')\n",
        "    \n",
        "    print(f\"‚úÖ Training dataset loaded: {len(train_dataset)} examples\")\n",
        "    print(f\"‚úÖ Validation dataset loaded: {len(val_dataset)} examples\")\n",
        "    \n",
        "    # Load metadata\n",
        "    with open('./processed_data/metadata.json', 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "    \n",
        "    print(f\"\\nüìä Dataset Statistics:\")\n",
        "    for key, value in metadata.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "        \n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Processed data not found. Please run 1_data_preparation.ipynb first.\")\n",
        "    raise\n",
        "\n",
        "# Display sample data\n",
        "print(f\"\\nüìù Sample Training Example:\")\n",
        "print(\"=\" * 60)\n",
        "print(train_dataset[0]['text'][:800] + \"...\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 3. Model and Tokenizer Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
        "OUTPUT_DIR = \"./fine_tuned_legal_mistral\"\n",
        "\n",
        "# QLoRA configuration for efficient training\n",
        "qlora_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "print(f\"üîÑ Loading model: {MODEL_NAME}\")\n",
        "print(\"Using QLoRA (4-bit quantization) for memory efficiency...\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(f\"‚úÖ Tokenizer loaded\")\n",
        "print(f\"  Vocab size: {len(tokenizer)}\")\n",
        "print(f\"  Pad token: {tokenizer.pad_token}\")\n",
        "\n",
        "# Load model with quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=qlora_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model loaded with 4-bit quantization\")\n",
        "print(f\"  Model device: {next(model.parameters()).device}\")\n",
        "print(f\"  Model dtype: {next(model.parameters()).dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. LoRA Configuration and Model Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA configuration for Mistral\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # Rank\n",
        "    lora_alpha=32,  # Alpha parameter for LoRA scaling\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
        "                   \"gate_proj\", \"up_proj\", \"down_proj\"],  # Mistral attention layers\n",
        "    lora_dropout=0.05,  # Dropout probability for LoRA layers\n",
        "    bias=\"none\",  # Bias type\n",
        "    task_type=TaskType.CAUSAL_LM,  # Task type\n",
        ")\n",
        "\n",
        "print(\"üîß LoRA Configuration:\")\n",
        "print(f\"  Rank (r): {lora_config.r}\")\n",
        "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
        "print(f\"  Target modules: {lora_config.target_modules}\")\n",
        "print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
        "\n",
        "# Get PEFT model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable_params = 0\n",
        "all_param = 0\n",
        "for _, param in model.named_parameters():\n",
        "    all_param += param.numel()\n",
        "    if param.requires_grad:\n",
        "        trainable_params += param.numel()\n",
        "\n",
        "print(f\"\\nüìä Model Parameters:\")\n",
        "print(f\"  Trainable params: {trainable_params:,}\")\n",
        "print(f\"  All params: {all_param:,}\")\n",
        "print(f\"  Trainable%: {100 * trainable_params / all_param:.2f}%\")\n",
        "\n",
        "# Enable gradient checkpointing for memory efficiency\n",
        "model.gradient_checkpointing_enable()\n",
        "model.config.use_cache = False  # Disable cache for training\n",
        "\n",
        "print(f\"\\n‚úÖ Model prepared for training with LoRA\")\n",
        "print(f\"  Gradient checkpointing: Enabled\")\n",
        "print(f\"  Cache: Disabled for training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5. Training Configuration\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
